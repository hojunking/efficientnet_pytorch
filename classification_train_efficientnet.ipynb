{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, magic, shutil\n",
    "from glob import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, gc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import sklearn\n",
    "#from skimage import io\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c0283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 0,\n",
    "    'model': 'efficientnet_lite0',\n",
    "    'img_size': 256,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 30,\n",
    "    'valid_bs': 10,\n",
    "    'T_0': 10,\n",
    "    'lr': 1e-4,\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay':1e-5,\n",
    "    'num_workers': 6,\n",
    "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed30653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =timm.create_model('efficientnet_lite0', pretrained=True, num_classes=144)\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4b3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2233a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c454bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path, train_arg=False, sub_path=None):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    if train_arg:\n",
    "        dimg = cv2.fastNlMeansDenoisingColored(im_rgb, None, 10,10,7,21)\n",
    "        return dimg\n",
    "    else:\n",
    "        return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[0]\n",
    "    H = size[1]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "class ColonDataset(Dataset):\n",
    "    def __init__(self, df, data_root, train_arg=False,\n",
    "                 transforms=None, \n",
    "                 output_label=True, \n",
    "                 one_hot_label=False,\n",
    "                 do_fmix=False, \n",
    "                 fmix_params={\n",
    "                     'alpha': 1., \n",
    "                     'decay_power': 3., \n",
    "                     'shape': (CFG['img_size'], CFG['img_size']),\n",
    "                     'max_soft': True, \n",
    "                     'reformulate': False\n",
    "                 },\n",
    "                 do_cutmix=False,\n",
    "                 cutmix_params={\n",
    "                     'alpha': 1,\n",
    "                 }\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.train_arg = train_arg\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.do_fmix = do_fmix\n",
    "        self.fmix_params = fmix_params\n",
    "        self.do_cutmix = do_cutmix\n",
    "        self.cutmix_params = cutmix_params\n",
    "        \n",
    "        self.output_label = output_label\n",
    "        self.one_hot_label = one_hot_label\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "            #print(self.labels)\n",
    "            \n",
    "            if one_hot_label is True:\n",
    "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
    "                #print(self.labels)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "        \n",
    "        img  = get_img(\"{}/{}\".format(self.data_root[index], self.df.loc[index]['image_id']), train_arg=self.train_arg)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            with torch.no_grad():\n",
    "                #lam, mask = sample_mask(**self.fmix_params)\n",
    "                \n",
    "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n",
    "                \n",
    "                # Make mask, get mean / std\n",
    "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
    "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
    "    \n",
    "                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "               \n",
    "                fmix_img  = get_img(\"{}/{}\".format(self.data_root[fmix_ix], self.df.iloc[fmix_ix]['image_id']), train_arg=self.train_arg)\n",
    "            \n",
    "\n",
    "                if self.transforms:\n",
    "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
    "\n",
    "                mask_torch = torch.from_numpy(mask)\n",
    "                \n",
    "                # mix image\n",
    "                img = mask_torch*img+(1.-mask_torch)*fmix_img\n",
    "\n",
    "                #print(mask.shape)\n",
    "\n",
    "                #assert self.output_label==True and self.one_hot_label==True\n",
    "\n",
    "                # mix target\n",
    "                rate = mask.sum()/CFG['img_size']/CFG['img_size']\n",
    "                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n",
    "                #print(target, mask, img)\n",
    "                #assert False\n",
    "        \n",
    "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            #print(img.sum(), img.shape)\n",
    "            with torch.no_grad():\n",
    "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                \n",
    "                cmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[cmix_ix]['image_id']), train_arg=self.train_arg)\n",
    "                \n",
    "                if self.transforms:\n",
    "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
    "                    \n",
    "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox((CFG['img_size'], CFG['img_size']), lam)\n",
    "\n",
    "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (CFG['img_size'] * CFG['img_size']))\n",
    "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
    "                \n",
    "            #print('-', img.sum())\n",
    "            #print(target)\n",
    "            #assert False\n",
    "                            \n",
    "        # do label smoothing\n",
    "        #print(type(img), type(target))\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed846a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, \n",
    "    ShiftScaleRotate, CenterCrop, Resize, SmallestMaxSize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            #SmallestMaxSize(CFG['img_size']),\n",
    "            #RandomResizedCrop(CFG['img_size'], CFG['img_size'], p=0.5),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),   # (h, w)\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "#             GaussNoise(p=0.7),\n",
    "#             HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1601bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcepImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class=144, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained, num_classes=n_class)\n",
    "        try :\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, n_class)\n",
    "\n",
    "        except :\n",
    "            \n",
    "            try : \n",
    "                n_features = self.model.fc.in_features\n",
    "                self.model.fc = nn.Linear(n_features, n_class)\n",
    "            \n",
    "            except :\n",
    "                try:\n",
    "                    n_features = self.model.head.fc.in_channels\n",
    "                    self.model.head.fc = nn.Conv2d(n_features,n_class,kernel_size=(1, 1), stride=(1, 1))\n",
    "            \n",
    "                except:\n",
    "                    n_features = self.model.head.in_features\n",
    "                    self.model.head = nn.Linear(n_features, n_class)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240e5531",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataloader\u001b[39m(df, trn_idx, val_idx, data_root\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mdir\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalyst\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BalanceClassSampler\n\u001b[1;32m      5\u001b[0m     train_ \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[trn_idx,:]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root=train.dir.values):\n",
    "    \n",
    "    from catalyst.data.sampler import BalanceClassSampler\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "    train_data_root = data_root[trn_idx]\n",
    "    valid_data_root = data_root[val_idx]\n",
    "    \n",
    "        \n",
    "    train_ds = ColonDataset(train_, train_data_root, transforms=get_train_transforms(), output_label=True, one_hot_label=False, do_fmix=False, do_cutmix=False)\n",
    "    valid_ds = ColonDataset(valid_, valid_data_root, transforms=get_valid_transforms(), output_label=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=False,        \n",
    "        num_workers=CFG['num_workers'],\n",
    "        sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"upsampling\")\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
    "    model.train()\n",
    "\n",
    "    t = time.time()\n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        #print(\"111\")\n",
    "        #print(image_labels.shape, exam_label.shape)\n",
    "        with autocast():\n",
    "            image_preds = model(imgs)   #output = model(input)\n",
    "            #print(image_preds.shape, exam_pred.shape)\n",
    "            #print(\"222\")\n",
    "            loss = loss_fn(image_preds, image_labels)\n",
    "            loss_sum+=loss\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01\n",
    "\n",
    "            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                if scheduler is not None and schd_batch_update:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                \n",
    "                pbar.set_description(description)\n",
    "            \n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "    #print(\"333\")\n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,image_preds_all)\n",
    "    accuracy = (image_preds_all==image_targets_all).mean()\n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    print('train accuracy = {:.5f}'.format(accuracy))\n",
    "    print(matrix)\n",
    "    \n",
    "    if scheduler is not None and not schd_batch_update:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return image_preds_all, accuracy, trn_loss, matrix\n",
    "\n",
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
    "    model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        #print(image_preds.shape, exam_pred.shape)\n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, image_labels)\n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]  \n",
    "        \n",
    "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "            pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,image_preds_all)\n",
    "    acc = (image_preds_all==image_targets_all).mean()\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    print('validation accuracy = {:.5f}'.format(acc))\n",
    "    print(matrix)\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        if schd_loss_update:\n",
    "            scheduler.step(loss_sum/sample_num)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            \n",
    "    return image_preds_all, acc, val_loss, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d9a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
    "class MyCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        lsm = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            lsm = lsm * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = -(targets * lsm).sum(-1)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9950b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37102a34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mdir\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m      \u001b[38;5;66;03m# for training only, need nightly build pytorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mproject_name, entity\u001b[38;5;241m=\u001b[39muser)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "model_dir = '{}'.format(CFG['model'])\n",
    "train_dir = train.dir.values\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    print(\"start training!\")\n",
    "#     wandb.login()\n",
    "#     wandb.init(project=\"3class classification\")\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Vlaid accuracy\", step_metric=\"epoch\")\n",
    "    \n",
    "    seed_everything(CFG['seed'])\n",
    "    print(\"wandb settings\")\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        break\n",
    "    \n",
    "    print(f'Training start with epoch {CFG[\"epochs\"]} \\n')\n",
    "    # trn_idx = 0\n",
    "    # val_idx = 0\n",
    "  \n",
    "    train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root=train_dir)\n",
    "\n",
    "    print(\"after loaddataset\")\n",
    "    device = torch.device(CFG['device'])\n",
    "\n",
    "    model = ForcepImgClassifier(CFG['model'], train.label.nunique(), pretrained=True).to(device)\n",
    "    print(\"before gradscaler\")\n",
    "    scaler = GradScaler()   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], \n",
    "#                                      weight_decay=CFG['weight_decay']\n",
    "                                )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=25)\n",
    "#         scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "#         scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=25, \n",
    "    #                                                max_lr=CFG['lr'], epochs=CFG['epochs'], steps_per_epoch=len(train_loader))\n",
    "    print(\"loss\")\n",
    "    loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    wandb.watch(model, loss_tr, log='all')\n",
    "    \n",
    "    train_acc_list = []\n",
    "    train_matrix_list = []\n",
    "    valid_acc_list = []\n",
    "    valid_matrix_list = []\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"start epochs!\")\n",
    "    for epoch in range(CFG['epochs']):\n",
    "\n",
    "        train_preds_all, train_acc, train_loss, train_matrix = train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, \n",
    "                                                                   scheduler=scheduler, schd_batch_update=False)\n",
    "        wandb.log({'Train Accuracy':train_acc, 'Train Loss' : train_loss, 'epoch' : epoch})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            valid_preds_all, valid_acc, valid_loss, valid_matrix= valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
    "            wandb.log({'Valid Accuracy':valid_acc, 'Valid Loss' : valid_loss, 'epoch' : epoch})\n",
    "            \n",
    "        torch.save(model.state_dict(), (model_dir+'/20%_drop_{}_{}').format(CFG['model'], epoch))\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_matrix_list.append(train_matrix)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        valid_matrix_list.append(valid_matrix)\n",
    "        \n",
    "        fig = plt.figure().add_subplot(1, 1, 1)\n",
    "        fig.plot(train_acc_list, label='Train Acc')\n",
    "        fig.plot(valid_acc_list, label='Valid Acc')\n",
    "        fig.set_title('Train-Vlaid accuracy')\n",
    "        fig.legend()\n",
    "        wandb.log({'Train-Vlaid accuracy': wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "    end = time.time() - start\n",
    "    time_ = str(datetime.timedelta(seconds=end)).split(\".\")[0]\n",
    "    print(\"time :\", time_)\n",
    "\n",
    "    best_index = valid_acc_list.index(max(valid_acc_list))\n",
    "    print(f'Best Epoch : {best_index}/ {len(valid_acc_list)}')\n",
    "    print(f'Best Train Accuracy : {train_acc_list[best_index]:.5f}')\n",
    "    print(train_matrix_list[best_index])\n",
    "    print(f'Best Valid Accuracy : {valid_acc_list[best_index]:.5f}')\n",
    "    print(valid_matrix_list[best_index])\n",
    "\n",
    "#     torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n",
    "\n",
    "    del model, optimizer, train_loader, val_loader, scaler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_preds_all = train_preds_all\n",
    "    valid_preds_all = valid_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b952e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44029c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
